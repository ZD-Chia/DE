ETL Function

    
def extract_table_to_df(tablename, db_engine):
  retrun pd.read_sql("SELECT * FROM {}.format(tablename), db_engine)
  
def split_columns_transform(df, column, pat, suffixes):
  #converts column into str and splits it on pat...

def load_df_into_dwh(film_df, tablename, schema, db_engine):
  return pd.to_sql(tablename, db_engine, schema=schema, if_exits="replace")

db_engines = {...} #needs to be configured 
def etl():
  #extract
  film_pdf = extract_table_to_df("film", db_engines["store"])
  #trnasform
  film_df = split_columns_transform(film_df, "rental_rate", ".",["_dollar", "_cents"])
  #Load
  load_df_into_dwh(film_df, "film", "store", db_engines["dwh"])
  
  
  
  DAG
  from airflow.models import DAG
  from airflow.operators.python_operator import PyhtonOperator
  
  dag = DAG(dag_id="etl_pipeline",
            schedule_interval = " 0 0 * * *")
            
  etl_task = PythonOperator(task_id="etl_task",
                            python_callable=etl,
                            dag=dag)
  etl_task.set_upstream(wait_for_this_task)
  
  
  
# 6th March 2023 
  #Create recommendations table
 #Transform data into 2 column couse_id and average_rating
  
  def transfrom_avg_rating(rating_data):
    avg_rating = rating_data.groupby('course_id').rating.mean()  #group by course_id and extract average rating per course
    sort_rating = avg_rating.sort_values(ascending = False).reset_index()  # Retrun sorted average ratings p4er course
    retrun sort_rating.
    
   # Extract the rating data into a DF
   rating_data = extract_rating_data(db_engines)
   
   # Use transofrm_avg_rating on the extracterd data and print results
   avg_rating_data = transform_avg_rating(rating_data)
   print(avg_rating_data)
   
   
 #filter out corrupt data
   
   #print out the number of missing values per column
   print(course_data.isnull().sum())
   
   # the transformation should fill in the missing values
   def transform_fill_programming_language(course_data):
    imputed = course_data.fillna({"programming_lanaguage": "R"})
    return imputed
    
   transformed = transform_fill_programming_language(course_data)
   
  # Print out the number of missing values per column of transformed
  print(transformed.isnull().sum())
 
 #Using the recommender transformation
  #complete the transformation function
  def transform_recommendations(avg_course_ratings, course_to_recommend):
  merged = courses_to_recommend.merge(avg_course_ratings)  #merged both Df
  grouped = merged.sort_values("rating", ascending=False).groupby("user_id")  #Sort values by rating and group by user_id
  recommendations = grouped.head(3).sort_values("user_id").reset.index()  #produce the top 3 values and sort by user_id
  final_recommendations = recommendations[["user_id", "course_id", "rating"]]  
  return final_recommendations  #return final_recommendations
  
  #use the function with the predefined DF objects
  recommendations = transform_recommendations(avg_course_rating, courses_to_recommend)
 
 #connect URI for postgra database
  connection_uri ="postgresql://repl:password@localhost:5432/dwh"
  db_engine = sqlalchemy.create_engine(connection_uri)
  
  def load_to+dwh(recommendations):
    recommendations.to_sql("recommendations", db_engine, if_exists="replace") #update if exists
 
 #defining the DAG
  #defining the DAG os it runs on a daily basis
  dag = DAG(dag_id="recommendations",
          schedule_interval = " 0 0 * * *")
          
  #make sure 'etl()' is called in the operator. Pass the correct kwargs
  task_recommendations = PythonOperator(
      task_id="recomendations_task",
      python_callable=etl,
      op_kwargs={"db_engines":db_engines},  #specifying "db_engines" as the key in the dictionary you provide to op_kwargs
      )
 
 #QUerying the recommendations
  def recommendations_for_user(user_id, threhold=4.5):
      query="""
      SELECT title, rating FROM recommendations 
      INNER JOIN courses ON courses.course_id= recommendations.course_id  #join with the courrses table
      WHERE user_id=%(user_id)s AND rating>%(threshold)s
      ORDER BY rating DESC
      """
      # add the threshold parameter
      predictions_df = pd.read_sql(query, db_engine, params = {"user_id": user_id,
                                                               "Threshold": threshold})
      return predictions_df.title.values                                                         
   # try the function
   print(recommendations_for_user(12, 4.65))
      
    
   
